# # Random Forests Model --------------------------------------------------
#
# amazon_random_forest_model <- rand_forest(mtry = tune(),
#                                           min_n = tune(),
#                                           trees = 500) %>%
#   set_engine("ranger") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_random_forest_model)
# # K-Nearest Neighbors Model ---------------------------------------------
#
# amazon_knn_model <- nearest_neighbor(neighbors = tune()) %>%
#   set_engine("kknn") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_knn_model)
# Naive Bayes Model -----------------------------------------------------
amazon_naive_bayes_model <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
amazon_preliminary_workflow <- workflow() %>%
add_recipe(amazon_cleanup_recipe) %>%
add_model(amazon_naive_bayes_model)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(),
smoothness(),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 3)
# splitting data into folds
folds <- vfold_cv(amazon_train,
v = 3,
repeats = 1)
# Without progress handler
cv_results <- amazon_preliminary_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
rlang::last_trace()
# Setup -----------------------------------------------------------------
# clearing everything
#
rm(list = ls())
# loading in libraries
library(tidyverse)
library(discrim)
library(tidymodels)
library(embed)
library(lme4)
library(vroom)
library(ranger)
# library(kknn)
# library(patchwork)
# library(janitor)
# # Progress handler ------------------------------------------------------
# library(progressr)
# library(purrr)
# handlers(global = T)
# handlers("progress")
# Reading In Data -------------------------------------------------------
# 1050 columns
amazon_train <- vroom("amazon-employee-access-challenge/train.csv") %>%
mutate(ACTION = factor(ACTION))
amazon_test <- vroom("amazon-employee-access-challenge/test.csv")
# Cleaning Data ---------------------------------------------------------
amazon_cleanup_recipe <- recipe(ACTION ~ .,
data = amazon_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_factor_predictors(), threshold = 0.001) %>%
step_lencode_mixed(all_factor_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors())
# # This code is used to ensure that the above recipe worked as intended.
# amazon_train_clean <- bake(prep(amazon_cleanup_recipe),
#      new_data = amazon_train)
# # summary(amazon_train_clean)
# View(amazon_train_clean)
# # EDA -------------------------------------------------------------------
#
# # getting an introductory view to our data
# DataExplorer::plot_intro(amazon_train_clean)
#
# # checking for missing values
# DataExplorer::plot_missing(amazon_train_clean)
# # there are none!
#
# # looking at distribution of yeses and no's
# ggplot(data = amazon_train_clean, mapping = aes(y = ROLE_FAMILY,
#                                                 fill = factor(ACTION))) +
#   geom_bar()
#
# # looking at number of unique values of each column
# num_unique <- function(x){
#   return(length(unique(x)))
# }
#
# amazon_train_clean %>%
#   apply(X = ., MARGIN = 2, FUN = num_unique) %>%
#   cbind(names(amazon_train_clean)) %>%
#   as.data.frame(.) %>%
#   ggplot(mapping = aes(x= as.numeric(.),
#                        y = V2)) +
#   geom_col(orientation = "y") +
#   labs(title = "Number of unique values per feature",
#        x = "Unique values",
#        y = "Feature")
# # Logistic Regression Model ---------------------------------------------
# # setting model
# amazon_logistic_regression_model <- logistic_reg() %>%
#   set_engine("glm")
#
# amazon_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_logistic_regression_model) %>%
#   fit(data = amazon_train)
# # Penalized linear regression model -------------------------------------
# amazon_penalized_logistic_model <- logistic_reg(mixture = tune(),
#                                                 penalty = tune()) %>%
#   set_engine("glmnet")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_penalized_logistic_model)
# # Random Forests Model --------------------------------------------------
#
# amazon_random_forest_model <- rand_forest(mtry = tune(),
#                                           min_n = tune(),
#                                           trees = 500) %>%
#   set_engine("ranger") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_random_forest_model)
# # K-Nearest Neighbors Model ---------------------------------------------
#
# amazon_knn_model <- nearest_neighbor(neighbors = tune()) %>%
#   set_engine("kknn") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_knn_model)
# Naive Bayes Model -----------------------------------------------------
amazon_naive_bayes_model <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
amazon_preliminary_workflow <- workflow() %>%
add_recipe(amazon_cleanup_recipe) %>%
add_model(amazon_naive_bayes_model)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 1)),
smoothness(range = c(0.5, 2)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 3)
# splitting data into folds
folds <- vfold_cv(amazon_train,
v = 3,
repeats = 1)
# Without progress handler
cv_results <- amazon_preliminary_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
install.packages("naivebayes")
# Setup -----------------------------------------------------------------
# clearing everything
#
rm(list = ls())
# loading in libraries
library(tidyverse)
library(discrim)
library(naivebayes)
library(tidymodels)
library(embed)
library(lme4)
library(vroom)
library(ranger)
# library(kknn)
# library(patchwork)
# library(janitor)
# # Progress handler ------------------------------------------------------
# library(progressr)
# library(purrr)
# handlers(global = T)
# handlers("progress")
# Reading In Data -------------------------------------------------------
# 1050 columns
amazon_train <- vroom("amazon-employee-access-challenge/train.csv") %>%
mutate(ACTION = factor(ACTION))
amazon_test <- vroom("amazon-employee-access-challenge/test.csv")
# Cleaning Data ---------------------------------------------------------
amazon_cleanup_recipe <- recipe(ACTION ~ .,
data = amazon_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_factor_predictors(), threshold = 0.001) %>%
step_lencode_mixed(all_factor_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors())
# # This code is used to ensure that the above recipe worked as intended.
# amazon_train_clean <- bake(prep(amazon_cleanup_recipe),
#      new_data = amazon_train)
# # summary(amazon_train_clean)
# View(amazon_train_clean)
# # EDA -------------------------------------------------------------------
#
# # getting an introductory view to our data
# DataExplorer::plot_intro(amazon_train_clean)
#
# # checking for missing values
# DataExplorer::plot_missing(amazon_train_clean)
# # there are none!
#
# # looking at distribution of yeses and no's
# ggplot(data = amazon_train_clean, mapping = aes(y = ROLE_FAMILY,
#                                                 fill = factor(ACTION))) +
#   geom_bar()
#
# # looking at number of unique values of each column
# num_unique <- function(x){
#   return(length(unique(x)))
# }
#
# amazon_train_clean %>%
#   apply(X = ., MARGIN = 2, FUN = num_unique) %>%
#   cbind(names(amazon_train_clean)) %>%
#   as.data.frame(.) %>%
#   ggplot(mapping = aes(x= as.numeric(.),
#                        y = V2)) +
#   geom_col(orientation = "y") +
#   labs(title = "Number of unique values per feature",
#        x = "Unique values",
#        y = "Feature")
# # Logistic Regression Model ---------------------------------------------
# # setting model
# amazon_logistic_regression_model <- logistic_reg() %>%
#   set_engine("glm")
#
# amazon_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_logistic_regression_model) %>%
#   fit(data = amazon_train)
# # Penalized linear regression model -------------------------------------
# amazon_penalized_logistic_model <- logistic_reg(mixture = tune(),
#                                                 penalty = tune()) %>%
#   set_engine("glmnet")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_penalized_logistic_model)
# # Random Forests Model --------------------------------------------------
#
# amazon_random_forest_model <- rand_forest(mtry = tune(),
#                                           min_n = tune(),
#                                           trees = 500) %>%
#   set_engine("ranger") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_random_forest_model)
# # K-Nearest Neighbors Model ---------------------------------------------
#
# amazon_knn_model <- nearest_neighbor(neighbors = tune()) %>%
#   set_engine("kknn") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_knn_model)
# Naive Bayes Model -----------------------------------------------------
amazon_naive_bayes_model <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
amazon_preliminary_workflow <- workflow() %>%
add_recipe(amazon_cleanup_recipe) %>%
add_model(amazon_naive_bayes_model)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 1)),
smoothness(range = c(0.5, 2)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 3)
# splitting data into folds
folds <- vfold_cv(amazon_train,
v = 3,
repeats = 1)
# Without progress handler
cv_results <- amazon_preliminary_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
# Setup -----------------------------------------------------------------
# clearing everything
#
rm(list = ls())
# loading in libraries
library(tidyverse)
library(discrim)
library(naivebayes)
library(tidymodels)
library(embed)
library(lme4)
library(vroom)
library(ranger)
# library(kknn)
# library(patchwork)
# library(janitor)
# # Progress handler ------------------------------------------------------
# library(progressr)
# library(purrr)
# handlers(global = T)
# handlers("progress")
# Reading In Data -------------------------------------------------------
# 1050 columns
amazon_train <- vroom("amazon-employee-access-challenge/train.csv") %>%
mutate(ACTION = factor(ACTION))
amazon_test <- vroom("amazon-employee-access-challenge/test.csv")
# Cleaning Data ---------------------------------------------------------
amazon_cleanup_recipe <- recipe(ACTION ~ .,
data = amazon_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_factor_predictors(), threshold = 0.001) %>%
step_lencode_mixed(all_factor_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors())
# # This code is used to ensure that the above recipe worked as intended.
# amazon_train_clean <- bake(prep(amazon_cleanup_recipe),
#      new_data = amazon_train)
# # summary(amazon_train_clean)
# View(amazon_train_clean)
# # EDA -------------------------------------------------------------------
#
# # getting an introductory view to our data
# DataExplorer::plot_intro(amazon_train_clean)
#
# # checking for missing values
# DataExplorer::plot_missing(amazon_train_clean)
# # there are none!
#
# # looking at distribution of yeses and no's
# ggplot(data = amazon_train_clean, mapping = aes(y = ROLE_FAMILY,
#                                                 fill = factor(ACTION))) +
#   geom_bar()
#
# # looking at number of unique values of each column
# num_unique <- function(x){
#   return(length(unique(x)))
# }
#
# amazon_train_clean %>%
#   apply(X = ., MARGIN = 2, FUN = num_unique) %>%
#   cbind(names(amazon_train_clean)) %>%
#   as.data.frame(.) %>%
#   ggplot(mapping = aes(x= as.numeric(.),
#                        y = V2)) +
#   geom_col(orientation = "y") +
#   labs(title = "Number of unique values per feature",
#        x = "Unique values",
#        y = "Feature")
# # Logistic Regression Model ---------------------------------------------
# # setting model
# amazon_logistic_regression_model <- logistic_reg() %>%
#   set_engine("glm")
#
# amazon_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_logistic_regression_model) %>%
#   fit(data = amazon_train)
# # Penalized linear regression model -------------------------------------
# amazon_penalized_logistic_model <- logistic_reg(mixture = tune(),
#                                                 penalty = tune()) %>%
#   set_engine("glmnet")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_penalized_logistic_model)
# # Random Forests Model --------------------------------------------------
#
# amazon_random_forest_model <- rand_forest(mtry = tune(),
#                                           min_n = tune(),
#                                           trees = 500) %>%
#   set_engine("ranger") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_random_forest_model)
# # K-Nearest Neighbors Model ---------------------------------------------
#
# amazon_knn_model <- nearest_neighbor(neighbors = tune()) %>%
#   set_engine("kknn") %>%
#   set_mode("classification")
#
# amazon_preliminary_workflow <- workflow() %>%
#   add_recipe(amazon_cleanup_recipe) %>%
#   add_model(amazon_knn_model)
# Naive Bayes Model -----------------------------------------------------
amazon_naive_bayes_model <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
amazon_preliminary_workflow <- workflow() %>%
add_recipe(amazon_cleanup_recipe) %>%
add_model(amazon_naive_bayes_model)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 1)),
smoothness(range = c(0.5, 2)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 3)
# splitting data into folds
folds <- vfold_cv(amazon_train,
v = 3,
repeats = 1)
# Without progress handler
cv_results <- amazon_preliminary_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
# # cross-validation with progress handler
# with_progress({
#
#   p <- progressor(steps = length(folds$splits))
#
#   cv_results <- amazon_preliminary_workflow %>%
#     tune_grid(resamples = folds,
#               grid = tuning_grid,
#               metrics = metric_set(roc_auc),
#               control = control_grid(
#                 extract = function(x) {
#                   p(message = "Fold complete")
#                 }
#               ))
# })
# pulling off best tuning parameter values
best_tuning_parameters <- cv_results %>%
select_best(metric = "roc_auc")
# Making final workflow -------------------------------------------------
# making final workflow
amazon_workflow <- amazon_preliminary_workflow %>%
finalize_workflow(best_tuning_parameters) %>%
fit(data = amazon_train)
# Making Predictions ----------------------------------------------------
amazon_predictions <- predict(amazon_workflow,
new_data = amazon_test,
type = "prob")
# format the predictions as a submission file
amazon_predictions_formatted <- amazon_predictions %>%
mutate(id = 1:length(.pred_1)) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
# Writing submission file -----------------------------------------------
vroom_write(x = amazon_predictions_formatted,
file = "./preds.csv",
delim = ",")
source("~/GitHub/AmazonEmployeeAccess/AmazonAnalysis.R", echo = TRUE)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 10)),
smoothness(range = c(0.1, 20)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 10)
tuning_grid
View(tuning_grid)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 10)),
smoothness(range = c(0.1, 5)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 10)
View(tuning_grid)
# Cross-validation ------------------------------------------------------
# grid of tuning parameters
tuning_grid <- grid_regular(Laplace(range = c(0, 10)),
smoothness(range = c(0.1, 3.1)),
# neighbors(),
#mtry(range = c(1, dim(amazon_train)[2] - 1)),
#min_n(),
# penalty(),
# mixture(),
levels = 11)
View(tuning_grid)
